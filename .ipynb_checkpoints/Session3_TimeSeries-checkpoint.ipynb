{
 "metadata": {
  "name": "",
  "signature": "sha256:ac198122626e7629882e4258bc457b6c91cd407c6349a86812f636eb77c01963"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkRed\">Econometrics II: Time Series Analysis (or how to code a crystal ball)</font></font>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Data Ventures Global | Spring 2015 | Session 3: Econometrics - Time Series"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Time series is a powerful field in statistics applied widely in operations research, engineering, weather forecasting, and finance. The basic tenet is fairly intuitive: tomorrow depends a lot on today. Any time series analysis at the most basic level has to capture resolution, i.e. time increments, and horizon, i.e. how far out of a timeframe we are considering. The farther out we try to model, the more uncertainty our computations must capture. Long horizons with high resolution are generally limited by the limited past data, often requiring aggregating various low resolution time series data. As an extreme example if we want to model 100-years out on minute-by-minute increments, we likely won't find data of several decades on minute increments. It is more likely that we will find data starting and ending on week by week or so, and we would consolidate this data to produce our forecasts. For simplicity, we assume that our dependent variable is always nonnegative (although our changes therein may not be). "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\">Time Series Models</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\"><b>RANDOM WALK</b></font></font>. This is the simplest of models, where aside from account for a psuedo-linear trend, the idea is that all values are otherwise at the mercy of chance. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\"><b>AUTOREGRESSIVE</b></font></font>. We will look at "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\"><b>GARCH</b></font></font>. We will look at "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\">Mini-Project: Weather Forecasts</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's play the weatherman. For our purposes we shall make a few assumptions, notably that like the stock market in the long run, temperatures keep increasing (because of global warming, not crazy investors). Working with in groups of three or four, you have two major tasks: scrape data from a weather source, and forecast today's low, high, and average temperatures based on past data. How well did your findings work out? Submit a `.ipynb` file alone or a brief `.py` file with your code and writeup to your module leader. We will go over the solutions in an optional half-hour session next week. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "<center><font face='Garamond'>\u00a9 Data Ventures</font></center>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}