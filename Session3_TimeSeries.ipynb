{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkRed\">Econometrics II: Time Series Analysis (or how to code a crystal ball)</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Time series is a powerful field in statistics applied widely in operations research, engineering, weather forecasting, and finance. The basic tenet is fairly intuitive: tomorrow depends a lot on today. Any time series analysis at the most basic level has to capture resolution, i.e. time increments, and horizon, i.e. how far out of a timeframe we are considering. The farther out we try to model, the more uncertainty our computations must capture. Long horizons with high resolution are generally limited by the limited past data, often requiring aggregating various low resolution time series data. As an extreme example if we want to model 100-years out on minute-by-minute increments, we likely won't find data of several decades on minute increments. It is more likely that we will find data starting and ending on week by week or so, and we would consolidate this data to produce our forecasts. For simplicity, we assume that our dependent variable is always nonnegative (although our changes therein may not be). "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\">Time Series Models</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\"><b>RANDOM WALK</b></font></font>. This is the simplest of models, where aside from account for a psuedo-linear trend, the idea is that all values are otherwise at the mercy of chance. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.corpus import brown\n",
      " \n",
      "# This is a fast and simple noun phrase extractor (based on NLTK)\n",
      "# Feel free to use it, just keep a link back to this post\n",
      "# http://thetokenizer.com/2013/05/09/efficient-way-to-extract-the-main-topics-of-a-sentence/\n",
      "# Create by Shlomi Babluki\n",
      "# May, 2013\n",
      " \n",
      " \n",
      "# This is our fast Part of Speech tagger\n",
      "#############################################################################\n",
      "brown_train = brown.tagged_sents(categories='news')\n",
      "regexp_tagger = nltk.RegexpTagger(\n",
      "    [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),\n",
      "     (r'(-|:|;)$', ':'),\n",
      "     (r'\\'*$', 'MD'),\n",
      "     (r'(The|the|A|a|An|an)$', 'AT'),\n",
      "     (r'.*able$', 'JJ'),\n",
      "     (r'^[A-Z].*$', 'NNP'),\n",
      "     (r'.*ness$', 'NN'),\n",
      "     (r'.*ly$', 'RB'),\n",
      "     (r'.*s$', 'NNS'),\n",
      "     (r'.*ing$', 'VBG'),\n",
      "     (r'.*ed$', 'VBD'),\n",
      "     (r'.*', 'NN')\n",
      "])\n",
      "unigram_tagger = nltk.UnigramTagger(brown_train, backoff=regexp_tagger)\n",
      "bigram_tagger = nltk.BigramTagger(brown_train, backoff=unigram_tagger)\n",
      "#############################################################################\n",
      " \n",
      " \n",
      "# This is our semi-CFG; Extend it according to your own needs\n",
      "#############################################################################\n",
      "cfg = {}\n",
      "cfg[\"NNP+NNP\"] = \"NNP\"\n",
      "cfg[\"NN+NN\"] = \"NNI\"\n",
      "cfg[\"NNI+NN\"] = \"NNI\"\n",
      "cfg[\"JJ+JJ\"] = \"JJ\"\n",
      "cfg[\"JJ+NN\"] = \"NNI\"\n",
      "#############################################################################\n",
      " \n",
      " \n",
      "class NPExtractor(object):\n",
      " \n",
      "    def __init__(self, sentence):\n",
      "        self.sentence = sentence\n",
      " \n",
      "    # Split the sentence into singlw words/tokens\n",
      "    def tokenize_sentence(self, sentence):\n",
      "        tokens = nltk.word_tokenize(sentence)\n",
      "        return tokens\n",
      " \n",
      "    # Normalize brown corpus' tags (\"NN\", \"NN-PL\", \"NNS\" > \"NN\")\n",
      "    def normalize_tags(self, tagged):\n",
      "        n_tagged = []\n",
      "        for t in tagged:\n",
      "            if t[1] == \"NP-TL\" or t[1] == \"NP\":\n",
      "                n_tagged.append((t[0], \"NNP\"))\n",
      "                continue\n",
      "            if t[1].endswith(\"-TL\"):\n",
      "                n_tagged.append((t[0], t[1][:-3]))\n",
      "                continue\n",
      "            if t[1].endswith(\"S\"):\n",
      "                n_tagged.append((t[0], t[1][:-1]))\n",
      "                continue\n",
      "            n_tagged.append((t[0], t[1]))\n",
      "        return n_tagged\n",
      " \n",
      "    # Extract the main topics from the sentence\n",
      "    def extract(self):\n",
      " \n",
      "        tokens = self.tokenize_sentence(self.sentence)\n",
      "        tags = self.normalize_tags(bigram_tagger.tag(tokens))\n",
      " \n",
      "        merge = True\n",
      "        while merge:\n",
      "            merge = False\n",
      "            for x in range(0, len(tags) - 1):\n",
      "                t1 = tags[x]\n",
      "                t2 = tags[x + 1]\n",
      "                key = \"%s+%s\" % (t1[1], t2[1])\n",
      "                value = cfg.get(key, '')\n",
      "                if value:\n",
      "                    merge = True\n",
      "                    tags.pop(x)\n",
      "                    tags.pop(x)\n",
      "                    match = \"%s %s\" % (t1[0], t2[0])\n",
      "                    pos = value\n",
      "                    tags.insert(x, (match, pos))\n",
      "                    break\n",
      " \n",
      "        matches = []\n",
      "        for t in tags:\n",
      "            if t[1] == \"NNP\" or t[1] == \"NNI\":\n",
      "            #if t[1] == \"NNP\" or t[1] == \"NNI\" or t[1] == \"NN\":\n",
      "                matches.append(t[0])\n",
      "        return matches\n",
      " \n",
      " \n",
      "# Main method, just run \"python np_extractor.py\"\n",
      "sentence = \"Dear S1RID1, Thank you for your help the other day in discussing those potassium tests. NN01 and I talked to several physicians, and I think we need to study complete blood counts. We looked through the med code dictionary, and I think we need all values related to the following tests: 50409 50410 50411 50412 We also are not sure whether these med codes cover both campuses or not. Can you let me know if it would be possible to request those med codes for all inpatient visits, 2006 through 2008? Thanks so much for all of your help. I really appreciate it! Sincerely, S1RID2.\"\n",
      "\n",
      "def main(sentence):\n",
      "    np_extractor = NPExtractor(sentence)\n",
      "    result = np_extractor.extract()\n",
      "    return result\n",
      " \n",
      "if __name__ == '__main__':\n",
      "        sentence = \"Dear S1RID1, Thank you for your help the other day in discussing those potassium tests. NN01 and I talked to several physicians, and I think we need to study complete blood counts. We looked through the med code dictionary, and I think we need all values related to the following tests: 50409 50410 50411 50412 We also are not sure whether these med codes cover both campuses or not. Can you let me know if it would be possible to request those med codes for all inpatient visits, 2006 through 2008? Thanks so much for all of your help. I really appreciate it! Sincerely, S1RID2.\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "main(sentence)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "['Dear S1RID1',\n",
        " 'Thank',\n",
        " 'potassium tests.',\n",
        " 'NN01',\n",
        " 'complete blood counts.',\n",
        " 'code dictionary',\n",
        " 'Thanks',\n",
        " 'Sincerely',\n",
        " 'S1RID2']"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# EXERCISE 1\n",
      "# let's describe \n",
      "# Econometrics 1: Regressions (keeping things normal and straight)\n",
      "# Natural Language Processing: (It's actually kind of unnatural)\n",
      "# Optimization: (can't get better than this)\n",
      "# Neural networks: (when code meets science)\n",
      "\n",
      "\n",
      "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
      "#         Lars Buitinck <L.J.Buitinck@uva.nl>\n",
      "# License: BSD 3 clause\n",
      "\n",
      "from __future__ import print_function\n",
      "from time import time\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.decomposition import NMF\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "\n",
      "n_samples = 2000\n",
      "n_features = 1000\n",
      "n_topics = 10\n",
      "n_top_words = 20\n",
      "\n",
      "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
      "# to filter out useless terms early on: the posts are stripped of headers,\n",
      "# footers and quoted replies, and common English words, words occurring in\n",
      "# only one document or in at least 95% of the documents are removed.\n",
      "\n",
      "t0 = time()\n",
      "print(\"Loading dataset and extracting TF-IDF features...\")\n",
      "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
      "                             remove=('headers', 'footers', 'quotes'))\n",
      "\n",
      "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features,\n",
      "                             stop_words='english')\n",
      "tfidf = vectorizer.fit_transform(dataset.data[:n_samples])\n",
      "print(\"done in %0.3fs.\" % (time() - t0))\n",
      "\n",
      "# Fit the NMF model\n",
      "print(\"Fitting the NMF model with n_samples=%d and n_features=%d...\"\n",
      "      % (n_samples, n_features))\n",
      "nmf = NMF(n_components=n_topics, random_state=1).fit(tfidf)\n",
      "print(\"done in %0.3fs.\" % (time() - t0))\n",
      "\n",
      "feature_names = vectorizer.get_feature_names()\n",
      "\n",
      "for topic_idx, topic in enumerate(nmf.components_):\n",
      "    print(\"Topic #%d:\" % topic_idx)\n",
      "    print(\" \".join([feature_names[i]\n",
      "                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
      "    print()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading dataset and extracting TF-IDF features...\n",
        "done in 213.607s."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Fitting the NMF model with n_samples=2000 and n_features=1000...\n",
        "done in 220.232s."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Topic #0:\n",
        "people just don think like know say did really make time way ve right going sure said got wrong didn\n",
        "\n",
        "Topic #1:\n",
        "windows file dos use using program window files problem help pc running application version drivers ftp screen available work ms\n",
        "\n",
        "Topic #2:\n",
        "game team year games win play season players runs ll good toronto defense division teams won better player goal best\n",
        "\n",
        "Topic #3:\n",
        "thanks know does mail advance hi info interested anybody email like looking help card need appreciated hello information list send\n",
        "\n",
        "Topic #4:\n",
        "god jesus bible does faith christian christ christians church believe life lord true religion love human man belief people good\n",
        "\n",
        "Topic #5:\n",
        "drive drives hard disk card software mac power apple pc problem computer memory external speed board internal problems work monitor\n",
        "\n",
        "Topic #6:\n",
        "10 00 space new 11 12 15 16 20 18 25 17 sale earth 13 93 22 24 23 years\n",
        "\n",
        "Topic #7:\n",
        "car bike good cars engine new power speed miles buy price used like year want driving area bought just models\n",
        "\n",
        "Topic #8:\n",
        "key chip government clipper encryption keys use public law enforcement secure phone data used security communications secret standard going clinton\n",
        "\n",
        "Topic #9:\n",
        "edu com banks soon internet send ftp university mail information article pub mac student contact cc server address email list\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "No handlers could be found for logger \"sklearn.datasets.twenty_newsgroups\"\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\"><b>AUTOREGRESSIVE</b></font></font>. We will look at "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\"><b>GARCH</b></font></font>. We will look at "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font face='Garamond'><font color=\"DarkBlue\">Mini-Project: Weather Forecasts</font></font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's play the weatherman. For our purposes we shall make a few assumptions, notably that like the stock market in the long run, temperatures keep increasing (because of global warming, not crazy investors). Working with in groups of three or four, you have two major tasks: scrape data from a weather source, and forecast today's low, high, and average temperatures based on past data. How well did your findings work out? Submit a `.ipynb` file alone or a brief `.py` file with your code and writeup to `harvard.dataventures@gmail.com`. We will go over the solutions in an optional half-hour session next week. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "<center><font face='Garamond'>\u00a9 Data Ventures</font></center>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}