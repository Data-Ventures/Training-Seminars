{
 "metadata": {
  "name": "",
  "signature": "sha256:b634d14948bed2d09cf08960644cf65288b51d4ca946be00fa4d940c675e1f00"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<font color=\"#900000\">Regressions (and Regularization): Taking the 'Straight' (and Simple) Paths</font>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Data Ventures Global | Spring 2015 | Session 1: Supervised Learning: Univariate and Multivariate Linear Regressions, Binary and Multiclass Classification with Logistic Regressions, Overfitting and Regularization"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font color=\"000099\">Introduction</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A small part of the large domain of supervised machine learning, regressions are possibly the most commonly used method for crunching data, and for good reason. MENTION CLT, LARGE SAMPLES?? Note that these conditions will not always be valid, and so regressions analyses will not be an invariably legitimate approach, though it is often a good first place to start. We go over univariate and multivariate linear regressions, binary and multiclass classification with logistic regression, and how to address the problem of overfitting in these scenarios with regualarization. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font color=\"000099\">Univariate Linear Regression</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In univariate linear regression, where the $x_i$'s are our input variables and the $y_i$'s are our output variables, our hypothesis function that we infer from the training set and use to predict over the test set is of the form $h_\\theta (x) = \\theta_0 + \\theta_1 x$. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<img src=\"http://upload.wikimedia.org/wikipedia/commons/3/3a/Linear_regression.svg\" alt=\"Linear Regression. Courtesy of Wikipedia.\">\n",
      "</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We want to fit the best possible straight line and to do do we must find parameters $\\theta_0$ and $\\theta_1$ such that we minimize the difference between $h_\\theta$ and $y$ in the training set. We essentially are faced with an optimization problem (optimization will be covered as a separately unit in the weeks to come), in which we seek to minimize the squared errors in the following objective function, or cost function $C$ over $n$ training examples. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>$C(\\theta_0, \\theta_1)$ = $\\min_{\\theta_0, \\theta_1}$ $\\frac{1}{2n} \\sum_{i=1}^{n} (h_\\theta(x^{(i)}) - y^{(i)})^2$</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can visualize this as descending the contours to find the minimum of a 3D surface plot (of course, in the case of a single parameter $\\theta_1$ this trivializes to finding the minimum of a parabolic function): \n",
      "\n",
      "<center>\n",
      "<img src=\"http://www.astro.umd.edu/~cychen/MATLAB/ContourPlots_05.png\" alt=\"Matlab Plot. Courtesy of the University of Maryland.\">\n",
      "</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Gradient Descent**. We can solve univariate linear regression problems in theory by calculus. We can also turn to an algorithm known as *gradient descent* in which we take iteratively evaluate the direction along which we can slope down the quickest until we converge to the global minimum (also the local minimum in this univariate example, as the cost function will be a simple convex surface). To use gradient descent, we must construct an algorithm that will *simultaneously* update all the parameters $\\theta_j$ with the assignment $\\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} C(\\theta_0, \\theta_1)$ for $j=0$ and $j=1$. The learning rate $\\alpha$ controls how large of a descent we take in each iteration of the algorithm. If $\\alpha$ is too small the algorithm may take a long time to converge and if it too large the algorithm may never converge since it is possible to keep overshooting our desired minimum each time. If we have already reached the minimum, the value of $\\alpha$ will not matter. Given that we have started off with an appropriate learning rate, there will be no need to adjust $\\alpha$ as we approach the minimum since the slope will be less steep and thus we take a smaller descent along each successive iteration of the algo. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are variations of gradient descent but for all purposes we will be using batch gradient descent, in which we use all of the training set rather than work with only parts of it in each iteration. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Normal Equations Method**. While gradient descent works quite well for massive data, we can non-iteratively find our desired parameters for the regression hypothesis by making use of linear algebra in what is known as the *normal equations method*. This will work quite well for multivariate linear regression, in which we will have to handle multiple features. Please see the next section. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font color=\"000099\">Multivariate Linear Regression</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Things get more interesting and far more useful when dealing with multiple features or attributes. While let visually intuitive, multivariate linear regression becomes an instant powerhouse for matrix algebra. Our hypothesis function now becomes $h_\\theta (x) = \\theta_0 + \\theta_1 x_1 + ... + \\theta_k x_k$ for $k$ features, or attributes. \n",
      "\n",
      "<center>\n",
      "<img src=\"http://www.mathworks.com/help/examples/stats/EstimateMultipleLinearRegressionCoefficientsExample_01.png\" alt=\"Two Attributes/Features in Linear Regression. Courtesy of Mathworks.\">\n",
      "</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Gradient Descent with Feature Scaling**. This is a conceptually straightforward extension in which we simultaneously update for all $j \\in {0, 1, ..., k}$ with $\\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} C(\\theta_0, ..., \\theta_k)$. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Normal Equations Method**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Polynomial Regression**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font color=\"000099\">Logistic Regression: Binary Classification</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font color=\"000099\">Logistic Regression: Multiclass Classification</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font color=\"000099\">Regularization to Reduce Overfitting</font>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "<font color=\"black\">Questions?</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "No seriously, let us help you. Please direct all questions on material covered to your session leader before proceeding to the next part. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<font color=\"900000\">Assignment 1: Medical Data</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now it is time to put your skills to the test. Please find a group of three students for this brief assignment, due right before the next session of `Data Ventures`. This will be the same group of students you will work with throughout the segments on supervised learning and must be a group of students entirely different from those who you choose to work with for the segments on unsupervised learning and other computational methods, as well as for the final project. Please submit one notebook per group (do not send the `.ipynb` but rather the `nbviewer` linked page to your module leader with all your group members listed in the body of the email). Assignments will be mostly be evaluated on computational accuracy. We will also provide feedback on your logic and request all code to be aptly commented. Incomplete assignments are regarded as zero submission, but assignments are short enough that we do not expect unfinished work. While topics covered will become increasingly more sophisticated, we will maintain brevity on future exercises as well. If you are confused about anything regarding this project or the concepts discussed, please reach out to us via email or Facebook. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Please place final results in the designated lines. If you choose to write in a language other than Python, you must comment out the code you submit below in addition to submitting separate .r or .m files (if using R or Matlab). We will provide support for other languages.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "<center><font face='Garamond'>\u00a9 Data Ventures</font></center>"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}